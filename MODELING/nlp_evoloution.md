### NLP Evolution

1. n-gram model
2. A neural probabilistic language model - Bengio et.
     + learned representations of words in a shared vector space( like modern word2vec)
     + Allowed generalization to unseen n-grams via embeddings
     + Avoided the curse of dimensionality n sparse n-gram models
     + Inspired later work on RNNs, LSTMs, and Transformers
     - 
4. RNNs
     + Handle variable -length context via hidden state
     + model sequential word order more naturally
     + 
